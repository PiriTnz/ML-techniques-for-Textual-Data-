{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIzM8KB-D1F_",
        "outputId": "655a9f5b-05e9-41f2-a81b-0da9cb63d996"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install afinn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IseentAnEFXd",
        "outputId": "016234bc-6a05-4e85-843f-f5977e9cb30e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting afinn\n",
            "  Downloading afinn-0.1.tar.gz (52 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/52.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: afinn\n",
            "  Building wheel for afinn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for afinn: filename=afinn-0.1-py3-none-any.whl size=53429 sha256=34e208df609235cd4f30fec0e11cf65a0a062c7ef20bd659651f5711f99f960e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b0/05/90/43f79196199a138fb486902fceca30a2d1b5228e6d2db8eb90\n",
            "Successfully built afinn\n",
            "Installing collected packages: afinn\n",
            "Successfully installed afinn-0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVOHXKEgCbsi",
        "outputId": "4c82b67a-da75-4d4b-988a-5b587c416897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Usual data representation and manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# NLTK is very useful for natural language applications\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# This will be used to tokenize sentences\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "# We use spacy for extracting useful information from English words\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm', disable= [\"parser\", \"tag\", \"entity\"])\n",
        "\n",
        "# This dictionary will be used to expand contractions (e.g. we'll -> we will)\n",
        "# Install and import the 'contractions' library\n",
        "from contractions import contractions_dict # This line imports the contractions_dict after installing the package.\n",
        "import re\n",
        "\n",
        "# Unicodedata will be used to remove accented characters\n",
        "import unicodedata\n",
        "\n",
        "# BeautifulSoup will be used to remove html tags\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Lexicon models\n",
        "from afinn import Afinn\n",
        "from nltk.corpus import sentiwordnet as swn\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Evaluation libraries\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import unicodedata\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from contractions import contractions_dict\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('movie_reviews.csv')  # Adjust the path if necessary\n",
        "\n",
        "# Step 3: Restrict dataset to 2,500 positive and 2,500 negative reviews\n",
        "# Check the number of positive and negative reviews available\n",
        "num_positive = len(df[df['sentiment'] == 1])\n",
        "num_negative = len(df[df['sentiment'] == 0])\n",
        "\n",
        "# Sample a maximum of available reviews, up to 2500\n",
        "positive_reviews = df[df['sentiment'] == 1].sample(min(num_positive, 2500), random_state=42)\n",
        "negative_reviews = df[df['sentiment'] == 0].sample(min(num_negative, 2500), random_state=42)\n",
        "\n",
        "# Combine and shuffle the restricted dataset\n",
        "restricted_df = pd.concat([positive_reviews, negative_reviews]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# ... (rest of your code remains the same)\n",
        "# Preprocessing functions\n",
        "# 1. Remove HTML tags\n",
        "def strip_html(text):\n",
        "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
        "\n",
        "# 2. Remove accented and special characters\n",
        "def remove_accented_chars(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "# 3. Expand contractions\n",
        "def expand_contractions(text, contractions_dict=contractions_dict):\n",
        "    contractions_pattern = re.compile(f\"({'|'.join(contractions_dict.keys())})\", flags=re.IGNORECASE)\n",
        "    def replace(match):\n",
        "        return contractions_dict[match.group(0).lower()]\n",
        "    return contractions_pattern.sub(replace, text)\n",
        "\n",
        "# 4. Tokenize and remove stopwords\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return ' '.join([token for token in tokens if token.lower() not in stopword_list])\n",
        "\n",
        "# Complete text cleaning function\n",
        "def clean_text(text):\n",
        "    text = strip_html(text)  # Remove HTML tags\n",
        "    text = remove_accented_chars(text)  # Remove accented characters\n",
        "    text = expand_contractions(text)  # Expand contractions\n",
        "    text = remove_stopwords(text)  # Remove stopwords\n",
        "    return text\n",
        "\n",
        "# Apply preprocessing to 'review' column\n",
        "restricted_df['cleaned_review'] = restricted_df['review'].apply(clean_text)\n",
        "\n",
        "# Display the first few rows of the processed dataset\n",
        "print(restricted_df[['review', 'cleaned_review']].head())\n",
        "\n",
        "# Save the restricted and preprocessed dataset (optional)\n",
        "restricted_df.to_csv(\"restricted_movie_reviews.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Za3NEhEaE0F1",
        "outputId": "4dfa2dcf-1878-4654-f3ba-28a08dc60e46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [review, cleaned_review]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: ## Task 2: Using Unsupervised Lexicon-based models\n",
        "# If you did not manage to pre-process the dataset. Feel free to download the pre-processed dataset below for the rest of this tutorial.\n",
        "# [normalized_movie_reviews_5000.csv](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4b729e35-259a-4469-a72e-e996fe267c08/normalized_movie_reviews_5000.csv)\n",
        "# ---\n",
        "# 1. Split the dataset into a training (resp. test) dataset containing the first 4,000 (resp. last 1,000) instances.\n",
        "# 2. Use the AFINN to score the polarity of the reviews. A review with a score superior or equal to zero will be considered positive and negative otherwise. What is the performance of this approach?\n",
        "# 3. Create a simple function to aggregate the scores of the most common synsets for the words in the reviews using the positive and negative scores returned by Wordnet. What is the performance of this approach?\n",
        "# 4. VADER (Valence Aware Dictionary for Sentiment Reasoning) is another model used for text sentiment analysis that i\n",
        "\n",
        "# Split the dataset\n",
        "train_df = restricted_df[:4000]\n",
        "test_df = restricted_df[4000:]\n",
        "\n",
        "# AFINN\n",
        "afinn = Afinn()\n",
        "def predict_afinn(review):\n",
        "    score = afinn.score(review)\n",
        "    return 1 if score >= 0 else 0\n",
        "\n",
        "train_preds_afinn = train_df['cleaned_review'].apply(predict_afinn)\n",
        "test_preds_afinn = test_df['cleaned_review'].apply(predict_afinn)\n",
        "\n",
        "print(\"AFINN - Train Performance:\")\n",
        "print(classification_report(train_df['sentiment'], train_preds_afinn))\n",
        "print(\"AFINN - Test Performance:\")\n",
        "print(classification_report(test_df['sentiment'], test_preds_afinn))\n",
        "\n",
        "\n",
        "# WordNet\n",
        "def predict_wordnet(review):\n",
        "    tokens = tokenizer.tokenize(review)\n",
        "    synset_scores = []\n",
        "    for token in tokens:\n",
        "        synsets = list(swn.senti_synsets(token))\n",
        "        if synsets:\n",
        "            scores = [s.pos_score() - s.neg_score() for s in synsets]\n",
        "            synset_scores.extend(scores)\n",
        "    return 1 if sum(synset_scores) >= 0 else 0\n",
        "\n",
        "train_preds_wordnet = train_df['cleaned_review'].apply(predict_wordnet)\n",
        "test_preds_wordnet = test_df['cleaned_review'].apply(predict_wordnet)\n",
        "\n",
        "print(\"WordNet - Train Performance:\")\n",
        "print(classification_report(train_df['sentiment'], train_preds_wordnet))\n",
        "print(\"WordNet - Test Performance:\")\n",
        "print(classification_report(test_df['sentiment'], test_preds_wordnet))\n",
        "\n",
        "\n",
        "# VADER\n",
        "vader = SentimentIntensityAnalyzer()\n",
        "def predict_vader(review):\n",
        "    scores = vader.polarity_scores(review)\n",
        "    return 1 if scores['compound'] >= 0 else 0\n",
        "\n",
        "train_preds_vader = train_df['cleaned_review'].apply(predict_vader)\n",
        "test_preds_vader = test_df['cleaned_review'].apply(predict_vader)\n",
        "\n",
        "print(\"VADER - Train Performance:\")\n",
        "print(classification_report(train_df['sentiment'], train_preds_vader))\n",
        "print(\"VADER - Test Performance:\")\n",
        "print(classification_report(test_df['sentiment'], test_preds_vader))"
      ],
      "metadata": {
        "id": "J6XuS__8IWku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: VADER (Valence Aware Dictionary for Sentiment Reasoning) is another model used for text sentiment analysis that is sensitive to both polarity (positive/negative) and intensity (strength) of emotion. VADER returns four sentiment scores compound, neg, neu and pos. Use the compound score to classify the reviews. What is the performance of this approach?\n",
        "\n",
        "# VADER performance is already calculated in the provided code.\n",
        "# The code already prints the classification report for VADER on both training and testing sets.\n",
        "# No further code is needed to answer the question, as the provided code already does so."
      ],
      "metadata": {
        "id": "PhE-_Ai5Imuz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usual data representation and manipulation libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# This will be used to tokenize sentences\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "# ML Models\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "# Libraries for feature engineering\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Word Embeddings & distances\n",
        "import gensim\n",
        "from scipy.spatial import distance\n",
        "\n",
        "# Evaluation libraries\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "ZZeZjLUwIp83"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: 1. Tokenize the train/test reviews and transform the sentiment into one-hot encoddings.\n",
        "# 2. Use `gensim.models.word2vec.Word2Vec` to initialise CBOW and Skip-gram word embeddings models (size 500) on the training dataset.\n",
        "# 3. Word embeddings of similar words should be closer. Use `distance.cosine` to check the distance between the words such as `cool`, `nice`, `bad`, `hate` and `okay` with your CBOW model.\n",
        "# 4. Create a function that will aggregate the numerical vectors for each word in a sentence.\n",
        "\n",
        "# Tokenize the reviews\n",
        "tokenizer = ToktokTokenizer()\n",
        "train_reviews_tokenized = [tokenizer.tokenize(review) for review in train_df['cleaned_review']]\n",
        "test_reviews_tokenized = [tokenizer.tokenize(review) for review in test_df['cleaned_review']]\n",
        "\n",
        "# Transform the sentiment into one-hot encodings\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore')\n",
        "train_sentiment_encoded = enc.fit_transform(train_df[['sentiment']]).toarray()\n",
        "test_sentiment_encoded = enc.transform(test_df[['sentiment']]).toarray()\n",
        "\n",
        "# Initialize CBOW and Skip-gram models\n",
        "cbow_model = gensim.models.Word2Vec(sentences=train_reviews_tokenized, vector_size=500, window=5, min_count=1, workers=4)\n",
        "skipgram_model = gensim.models.Word2Vec(sentences=train_reviews_tokenized, vector_size=500, window=5, min_count=1, workers=4, sg=1)\n",
        "\n",
        "\n",
        "# Calculate cosine distances for specific words (CBOW)\n",
        "words = ['cool', 'nice', 'bad', 'hate', 'okay']\n",
        "for i in range(len(words)):\n",
        "    for j in range(i + 1, len(words)):\n",
        "        word1, word2 = words[i], words[j]\n",
        "        if word1 in cbow_model.wv and word2 in cbow_model.wv:\n",
        "            dist = distance.cosine(cbow_model.wv[word1], cbow_model.wv[word2])\n",
        "            print(f\"Cosine distance between '{word1}' and '{word2}': {dist}\")\n",
        "        else:\n",
        "            print(f\"One or both words not found in vocabulary for '{word1}' and '{word2}'\")\n",
        "\n",
        "\n",
        "def aggregate_vectors(sentence, model):\n",
        "    # Check if the sentence is empty\n",
        "    if not sentence:\n",
        "        return np.zeros(model.vector_size)  # Return a zero vector for empty sentences\n",
        "\n",
        "    vector_sum = np.zeros(model.vector_size)\n",
        "    word_count = 0\n",
        "    for word in sentence:\n",
        "      if word in model.wv:\n",
        "        vector_sum += model.wv[word]\n",
        "        word_count += 1\n",
        "\n",
        "    if word_count > 0 :\n",
        "        return vector_sum / word_count\n",
        "    else:\n",
        "        return vector_sum"
      ],
      "metadata": {
        "id": "Sk_uL2g7IvLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Step 1: Load the dataset with correct delimiter and check data\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/restricted_movie_reviews.csv\", delimiter=',')  # Confirm delimiter\n",
        "except Exception as e:\n",
        "    print(f\"Error loading dataset: {e}\")\n",
        "\n",
        "# Confirm the shape and columns of the DataFrame\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns)\n",
        "\n",
        "# Check if DataFrame is empty\n",
        "if df.empty:\n",
        "    print(\"Error: Dataset is empty.\")\n",
        "else:\n",
        "    # Confirm columns 'cleaned_review' and 'sentiment' are present\n",
        "    if 'cleaned_review' not in df.columns or 'sentiment' not in df.columns:\n",
        "        print(\"Error: 'cleaned_review' or 'sentiment' column not found.\")\n",
        "    else:\n",
        "        # Step 5: Split dataset into train and test sets\n",
        "        train_reviews, test_reviews, train_labels, test_labels = train_test_split(\n",
        "            df['cleaned_review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "        # Tokenize text for Word2Vec model\n",
        "        train_tokens = [review.split() for review in train_reviews]\n",
        "        test_tokens = [review.split() for review in test_reviews]\n",
        "\n",
        "        # Step 5a: Generate Word2Vec vectors using CBOW\n",
        "        cbow_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=1, sg=0)\n",
        "        # Step 5b: Generate Word2Vec vectors using Skip-gram\n",
        "        skipgram_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=1, sg=1)\n",
        "\n",
        "        # Helper function to generate review vectors\n",
        "        def get_review_vectors(tokens, model):\n",
        "            vectors = []\n",
        "            for review in tokens:\n",
        "                review_vec = [model.wv[word] for word in review if word in model.wv]\n",
        "                if review_vec:\n",
        "                    vectors.append(np.mean(review_vec, axis=0))  # Average word vectors\n",
        "                else:\n",
        "                    vectors.append(np.zeros(model.vector_size))\n",
        "            return np.array(vectors)\n",
        "\n",
        "        # Generate vectors for train and test sets using CBOW and Skip-gram\n",
        "        train_vectors_cbow = get_review_vectors(train_tokens, cbow_model)\n",
        "        test_vectors_cbow = get_review_vectors(test_tokens, cbow_model)\n",
        "        train_vectors_skipgram = get_review_vectors(train_tokens, skipgram_model)\n",
        "        test_vectors_skipgram = get_review_vectors(test_tokens, skipgram_model)\n",
        "\n",
        "        # Convert labels to categorical format\n",
        "        train_labels = to_categorical(train_labels, num_classes=2)\n",
        "        test_labels = to_categorical(test_labels, num_classes=2)\n",
        "\n",
        "        # Step 6: Build neural network model\n",
        "        def build_model(input_dim):\n",
        "            model = Sequential([\n",
        "                Dense(128, activation='relu', input_dim=input_dim),\n",
        "                Dropout(0.5),\n",
        "                Dense(64, activation='relu'),\n",
        "                Dropout(0.5),\n",
        "                Dense(2, activation='softmax')\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        # Create models for CBOW and Skip-gram Word2Vec features\n",
        "        model_cbow = build_model(train_vectors_cbow.shape[1])\n",
        "        model_skipgram = build_model(train_vectors_skipgram.shape[1])\n",
        "\n",
        "        # Step 7: Train the CBOW model\n",
        "        print(\"Training CBOW model...\")\n",
        "        history_cbow = model_cbow.fit(train_vectors_cbow, train_labels,\n",
        "                                       epochs=10, batch_size=32, validation_data=(test_vectors_cbow, test_labels))\n",
        "\n",
        "        # Train the Skip-gram model\n",
        "        print(\"Training Skip-gram model...\")\n",
        "        history_skipgram = model_skipgram.fit(train_vectors_skipgram, train_labels,\n",
        "                                              epochs=10, batch_size=32, validation_data=(test_vectors_skipgram, test_labels))\n",
        "\n",
        "        # Evaluate both models on test data\n",
        "        cbow_loss, cbow_accuracy = model_cbow.evaluate(test_vectors_cbow, test_labels, verbose=0)\n",
        "        skipgram_loss, skipgram_accuracy = model_skipgram.evaluate(test_vectors_skipgram, test_labels, verbose=0)\n",
        "\n",
        "        print(f\"CBOW Model - Test Accuracy: {cbow_accuracy:.4f}, Test Loss: {cbow_loss:.4f}\")\n",
        "        print(f\"Skip-gram Model - Test Accuracy: {skipgram_accuracy:.4f}, Test Loss: {skipgram_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjZ8KXDNJAC6",
        "outputId": "3dc154ce-5fff-43ae-d8a8-d280d1219eec"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (0, 3)\n",
            "Columns: Index(['review', 'sentiment', 'cleaned_review'], dtype='object')\n",
            "Error: Dataset is empty.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset and check if it's loading correctly\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/restricted_movie_reviews.csv\")\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: The file was not found. Please check the file path.\")\n",
        "except pd.errors.EmptyDataError:\n",
        "    print(\"Error: The file is empty.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Check the shape and column names to confirm data is loaded\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns)\n",
        "\n",
        "# Proceed if DataFrame is not empty and required columns are present\n",
        "if not df.empty and 'cleaned_review' in df.columns and 'sentiment' in df.columns:\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from tensorflow.keras.models import Sequential\n",
        "    from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    from collections import Counter\n",
        "    import numpy as np\n",
        "\n",
        "    # Step 5: Split dataset into train and test sets\n",
        "    train_reviews, test_reviews, train_labels, test_labels = train_test_split(\n",
        "        df['cleaned_review'], df['sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "    # Tokenize text and create vocabulary\n",
        "    PAD_INDEX = 0\n",
        "    NOT_FOUND_INDEX = 1\n",
        "    train_tokens = [review.split() for review in train_reviews]\n",
        "    all_tokens = [token for review in train_tokens for token in review]\n",
        "    token_counts = Counter(all_tokens)\n",
        "    vocab = {word: i+2 for i, (word, _) in enumerate(token_counts.items())}\n",
        "    vocab[\"<PAD>\"] = PAD_INDEX\n",
        "    vocab[\"<NOT_FOUND>\"] = NOT_FOUND_INDEX\n",
        "\n",
        "    # Find the length of the longest review\n",
        "    max_length = max(len(review) for review in train_tokens)\n",
        "    print(f\"Max review length: {max_length}\")\n",
        "\n",
        "    # Convert reviews to vectors of size `max_length`\n",
        "    def encode_review(review, vocab, max_length):\n",
        "        return [\n",
        "            vocab.get(word, NOT_FOUND_INDEX) for word in review.split()\n",
        "        ]\n",
        "\n",
        "    train_encoded = [encode_review(review, vocab, max_length) for review in train_reviews]\n",
        "    test_encoded = [encode_review(review, vocab, max_length) for review in test_reviews]\n",
        "    train_padded = pad_sequences(train_encoded, maxlen=max_length, padding=\"post\", value=PAD_INDEX)\n",
        "    test_padded = pad_sequences(test_encoded, maxlen=max_length, padding=\"post\", value=PAD_INDEX)\n",
        "\n",
        "    # Convert labels to categorical format\n",
        "    train_labels = to_categorical(train_labels, num_classes=2)\n",
        "    test_labels = to_categorical(test_labels, num_classes=2)\n",
        "\n",
        "    # Build LSTM model\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=len(vocab), output_dim=128, input_length=max_length),\n",
        "        SpatialDropout1D(0.2),\n",
        "        LSTM(64),\n",
        "        Dense(2, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(train_padded, train_labels, epochs=5, batch_size=32, validation_data=(test_padded, test_labels))\n",
        "\n",
        "    # Evaluate model performance\n",
        "    loss, accuracy = model.evaluate(test_padded, test_labels, verbose=0)\n",
        "    print(f\"LSTM Model - Test Accuracy: {accuracy:.4f}, Test Loss: {loss:.4f}\")\n",
        "else:\n",
        "    print(\"Dataset is empty or missing required columns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6rs_5YgK2iJ",
        "outputId": "6218d034-67f1-484d-a10b-cee8a1695efa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully.\n",
            "Dataset shape: (0, 3)\n",
            "Columns: Index(['review', 'sentiment', 'cleaned_review'], dtype='object')\n",
            "Dataset is empty or missing required columns.\n"
          ]
        }
      ]
    }
  ]
}